{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc755bbb52cc4659a9a96d7e06b246ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9131e2212cd3485886e1bbab92fe66d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import PDMnist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_dataset = PDMnist(data_dir='data', train=True, num_filtrations=10, leave=32)\n",
    "test_dataset = PDMnist(data_dir='data', train=False, num_filtrations=10, leave=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'vmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ada2907a0eca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPersistenceWeightedSlicedWassersteinLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChamferLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHausdorffLoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mnist_dir_filtration\\losses.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwasserstein_1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msliced_wasserstein_distance_batched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliced_wasserstein_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mweighted_sliced_wasserstein_distance_batched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_projections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'vmap'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "def val_step(model, valloader, device):\n",
    "    model.eval()\n",
    "    metric_chamfer = 0.\n",
    "    metric_hausdorff = 0.\n",
    "    val_len = len(valloader.dataset)\n",
    "    for X, Z, v in valloader:\n",
    "        with torch.no_grad():\n",
    "            Z = Z[..., :2].to(torch.float32).to(device)\n",
    "            Z_hat = model(X.to(device))\n",
    "        \n",
    "        metric_chamfer += ChamferLoss(reduce='sum')(Z_hat, Z)\n",
    "        metric_hausdorff += HausdorffLoss(reduce='sum')(Z_hat, Z)\n",
    "    wandb.log({'val_chamfer': metric_chamfer / val_len, 'val_hausdorff': metric_hausdorff / val_len})\n",
    "\n",
    "def train_loop(model, trainloader, valloader, optimizer, loss_fn, device, scheduler=None, n_epochs=25):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    loss_fn = PersistenceWeightedSlicedWassersteinLoss(q=1, reduce=\"sum\", random_seed=0)\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for X, Z, v in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Z = Z[..., :2].to(torch.float32).to(device)\n",
    "            Z_hat = model(X.to(device))\n",
    "            loss = loss_fn(Z_hat, Z)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            total_norm = 0\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None and p.requires_grad]\n",
    "            for p in parameters:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({'loss': loss, 'grad_norm': total_norm, 'learning rate': lr})\n",
    "        \n",
    "        val_step(model, valloader, device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ImageSet2Set\n",
    "from losses import PersistenceWeightedSlicedWassersteinLoss, ChamferLoss, HausdorffLoss\n",
    "import wandb\n",
    "\n",
    "hyperparams = {\n",
    "    \"model\": {\n",
    "        \"n_out_max\": 32,\n",
    "        \"d_in\": 2,\n",
    "        \"d_out\": 2,\n",
    "        \"d_hidden\": 64,\n",
    "        \"d_mlp\": 256,\n",
    "    },\n",
    "    \"lr\": 0.00005,\n",
    "    \"n_steps_warmup\": 5000,\n",
    "    \"n_epochs\": 40\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, num_workers=2, shuffle=True, drop_last=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)\n",
    "\n",
    "model = ImageSet2Set(**hyperparams[\"model\"]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"lr\"])\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.2, total_iters=hyperparams[\"n_steps_warmup\"])\n",
    "\n",
    "loss_fn = PersistenceWeightedSlicedWassersteinLoss(q=1, reduce=\"sum\", random_seed=0)\n",
    "\n",
    "run = \"mnist_10_filt_fixed_full\"\n",
    "wandb.login(key='bbe60953ed99662c4459f461386ecd58a2f2ee3a')\n",
    "wandb.init(project=\"mnist_pds\", \n",
    "           name=f\"experiment_{run}\",\n",
    "           config=hyperparams\n",
    ")\n",
    "\n",
    "\n",
    "final_model = train_loop(model, trainloader, testloader, optimizer, loss_fn, device, scheduler, n_epochs=hyperparams[\"n_epochs\"])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
